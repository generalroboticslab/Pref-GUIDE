# Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Rewards
[Zhengran Ji](https://jzr01.github.io/)ยน, [Boyuan Chen](http://boyuanchen.com/)ยน

ยน Duke University

[website](http://generalroboticslab.com/Pref-GUIDE) | [paper](https://arxiv.org/abs/2508.07126) | [video](video_link)

## Overview
![Method](Figure/Method1.png)

## Result
![Method](Figure/Main_Experiment.png)



## Quick Start

Clone the repository:
```bash
git clone https://github.com/generalroboticslab/Pref-GUIDE.git
```

## Acknowledgement


This work is supported by the ARL STRONG program under awards W911NF2320182, W911NF2220113, and W911NF2420215, and by gift supports from BMW and OpenAI. We also thank [Lingyu Zhang](https://lingyu98.github.io/) for helpful discussion.


## Citation

If you think this paper is helpful, please consider citing our work

```plaintext
@ARTICLE{2025arXiv250807126J,
       author = {{Ji}, Zhengran and {Chen}, Boyuan},
        title = "{Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning}",
      journal = {arXiv e-prints},
     keywords = {Machine Learning, Artificial Intelligence},
         year = 2025,
        month = aug,
          eid = {arXiv:2508.07126},
        pages = {arXiv:2508.07126},
archivePrefix = {arXiv},
       eprint = {2508.07126},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2025arXiv250807126J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
```

