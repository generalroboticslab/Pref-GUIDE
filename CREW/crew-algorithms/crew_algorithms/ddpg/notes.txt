human feedback: data[("agents", "observation", "obs_0_1")]
shape: [10]:
    0: scalar feedback
    1: whether on IL (bool)
    2: teleoperation action (0-3)
    3: trajID (int)
    456: player pos
    789: player rot
    101112: treas po


DiscreteTensorSpec(
    shape=torch.Size([1, 1]),
    space=DiscreteBox(n=5),
    device=cuda,
    dtype=torch.int64,
    domain=discrete)


obs_4, act_4: 386


{0: 1.1166, 1: 0.1083, 2: 0.4877, 3: 1.5594, 4: 0.6726, 5: 0.8583, 6: 0.0, 7: 1.1674, 8: 0.6304, 9: 1.7679, 10: 0.5096, 11: 0.1071, 12: 0.0885, 13: 0.8014, 14: 0.4572, 15: 0.3584, 16: 1.5693, 17: 1.293, 18: 0.6867, 19: 0.2247, 20: 1.8594, 21: 0.387, 22: 0.0672, 23: 0.744, 24: 0.8431, 25: 0.8457, 26: 0.7297, 27: 0.4763, 28: 0.8921, 29: 0.4871, 30: 0.645, 31: 0.4324, 32: 1.7972, 33: 0.4231, 34: 0.3485, 35: 0.0126, 36: 2.0637, 37: 0.3587, 38: 0.114, 39: 0.897, 40: 0.3388, 41: 0.5022, 42: 0.5006, 43: 0.5522, 44: 0.7869, 45: 0.8302, 46: 0.1804, 47: 0.0112, 48: 0.8268, 49: 0.9163, 50: 1.5442, 51: 0.1749, 52: 1.5035, 53: 0.9786, 54: 0.8853, 55: 0.2599, 56: 0.2939, 57: 1.6948, 58: 0.9834, 59: 0.332, 60: 0.7411, 61: 0.6942, 62: 0.493, 63: 0.4332, 64: 0.2413, 65: 0.8697, 66: 0.5758, 67: 0.6153, 68: 1.1735, 69: 0.1506, 70: 0.6858, 71: 0.681, 72: 0.6987, 73: 0.9289, 74: 0.7255, 75: 1.8765, 76: 1.3948, 77: 0.7938, 78: 0.7486, 79: 0.4312, 80: 1.6549, 81: 0.8332, 82: 1.8309, 83: 0.2097, 84: 0.6979, 85: 0.0056, 86: 1.7658, 87: 0.6729, 88: 0.0123, 89: 0.9856, 90: 0.5624, 91: 0.8184, 92: 0.1344, 93: 1.4883, 94: 1.3359, 95: 1.2212, 96: 0.324, 97: 1.2888, 98: 0.0415, 99: 0.7272}
num_success:  20
Accumulated Reward 74.5478
L_actor=-0.401| L_Q=0.138


changes made to torhcrl code:
1. envs: when agent doesn't request decision, set to return null
2. FrameSkipTransform deal with done signal:
      def _step(self, tensordict: TensorDictBase) -> TensorDictBase:
        parent = self.parent
        if parent is None:
            raise RuntimeError("parent not found for FrameSkipTransform")
        reward_key = parent.reward_key
        done_key = parent.done_key
        reward = tensordict.get(("next", reward_key))
        done = tensordict.get(("next", done_key))
        # print(tensordict.get(("agents", "action")).item(), reward.item(), done.item(), -1)
        if done:
            return tensordict

        for _ in range(self.frame_skip - 1):
            tensordict = parent._step(tensordict)
            reward = reward + tensordict.get(("next", reward_key))
            done = tensordict.get(("next",done_key))
            # print(tensordict.get(("agents", "action")).item(), reward.item(), done.item(), _)
            if done:
                break
        return tensordict.set(("next", reward_key), reward)



stage 1: (1201_2319)
1226 traj, 10805s = 108k steps
lr=3e-4


stage 2:

utd=0.5: 1202_1800
utd=2.0: 1202_1809

utd=1.0, entropy=0.5: 1202_2312
utd=1.0, entropy=0.98: 1202_2322

add dense: 1203_0929
no obs: 1203_1821

1204_0327: frames_per_batch=500, batch_size=256, model architecture, UTD=0.5
1204_0854: random shift
1204_1451: stage1, more obvious rotation, random shift, shorter episodes
1204_1805: stage1. without random shift
1204_1714: stage2, 15s/eps

1205_0324: stage1.5
1205_0836: agent also inside C
1205_1038: smaller LR
1205_1220: stage 1.5 scratch


1205_1735: stage 1.5 scratch, mobilenet

1205_2159: stage 1.5 scratch, mobilenet, encoder lr=1e-5x
1205_2202: stage 1.5 scratch, ent=0.05

1206_0154: stage 1.5 scratch, mobilenet, utd 0.05

1206_0611: stage 1.5 scratch, adam eps=1e-4

1206_1135: stage 1.5 scratch, 100timesR

1206_1438: 4 layer CNN

1206_1650: entropy=0.5
1206_1834: stage 1.5, fully observable (ablation for black region. effectively the same for stage 1-2)


1206_2133: large shift
1206_2338: entropy=0.98
1207_1652: grad clip 10 (BEST)
1207_1659: actor can't update enc
1207_1729: aug only at train

1208_0303: rotation
1208_0309: resnet18 nopretrain
1208_1217: gradclip40

1209_0147: ent0.4, rot, lr1e-4, gradclip10
1209_0207: ent0.4, rot, lr1e-4, gradclip10, 100R
1209_0208: ent0.4, rot, lr1e-4, gradclip40

####bad
1209_1503: new base (lr 1e-3, layernorm, clip10, rot)
1209_1513: (ent0.5) (CREW1)
1209_1518: 100R (CREW2)
#####

1209_2124: reproduce 1207_1659 (lab PC)
1209_2129: rot clip (CREW1)
1209_2131: rot, no clip (CREW2)


1210_1329: base
1210_1343: 100R
1210_1346: gradclip10

1211_0318: clip10, bs 256
1211_0320: clip50, bs 256
1211_0322: clip10, bs 256, norot


1211_1834: fpb 500
1211_2054: fpb 500, stage 2
1211_2131: fpb 500, stage 2, clip50


current exps: 
base: failnodone (add the line), clip
Lab PC:      stg2, states (continued)
CREW ROOM 1: stg3, states
CREW ROOM 2: stg2, full obs
CREW ROOM 3:  stg2, states (rerun)
---------
labPC: stg3 fullobs
CREW1: stg3 states / rerun
CREW2: stg2 fullobs / rerun
CREW3: stf2 states / rerun


=======
