Evaluation Command:
DeepTamer
python crew_algorithms/deep_tamer/eval.py envs=bowling envs.time_scale=1 exp_path=deep_tamer/PATH_TO_SAVED
python crew_algorithms/deep_tamer/eval.py envs=find_treasure envs.time_scale=10 exp_path=deep_tamer/PATH_TO_SAVED
python crew_algorithms/deep_tamer/eval.py envs=hide_and_seek_1v1 envs.time_scale=10 exp_path=deep_tamer/PATH_TO_SAVED

GUIDE:
python crew_algorithms/ddpg/eval.py envs=bowling envs.time_scale=1 exp_path=guide/PATH_TO_SAVED
python crew_algorithms/ddpg/eval.py envs=find_treasure envs.time_scale=10 exp_path=guide/PATH_TO_SAVED
python crew_algorithms/ddpg/eval.py envs=hide_and_seek_1v1 envs.time_scale=10 exp_path=guide/PATH_TO_SAVED


Human feedback training commands (first 5/10minutes). Put this into human-subject experiment code:
python crew_algorithms/ddpg envs=bowling collector.frames_per_batch=120 batch_size=120 hf=True feedback_model=True
python crew_algorithms/ddpg envs=find_treasure collector.frames_per_batch=240 batch_size=240 hf=True feedback_model=True
python crew_algorithms/ddpg envs=hide_and_seek_1v1 collector.frames_per_batch=240 batch_size=240 hf=True feedback_model=True

Continue Training Command (second 10/20minutes, no human):
python crew_algorithms/ddpg envs=bowling collector.frames_per_batch=120 batch_size=120 hf=True feedback_model=True continue_training=PATH_TO_SAVED
python crew_algorithms/ddpg envs=find_treasure collector.frames_per_batch=240 batch_size=240 hf=True feedback_model=True continue_training=PATH_TO_SAVED
python crew_algorithms/ddpg envs=hide_and_seek_1v1 collector.frames_per_batch=240 batch_size=240 hf=True feedback_model=True continue_training=PATH_TO_SAVED




evals:
5.14 
    [1, 2, 3, 4, 7, 12, 21, 22, 27]
5.15 
    LabPC[10, 11, 14, 19, 23, 26, 30], 
    ZrPC[31, 32, 33, 36, 37, 38, 39]
5.16
    LabPC[00, 08, 17, 18, 25], [05, 28]
    ZrPC[35, 40, 41]
5.17



potential bad data: 21, 51